<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Sketching a Classical Thermodynamic Theory of Information for MRI Reconstructions &#8212; Monalisa 0.1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="_static/custom-navigation.css?v=899df2ba" />
    <link rel="stylesheet" type="text/css" href="_static/custom-button.css?v=35775deb" />
    <link rel="stylesheet" type="text/css" href="_static/important.css?v=8e75c088" />
    <link rel="stylesheet" type="text/css" href="_static/tip.css?v=66ef22ed" />
    <script src="_static/documentation_options.js?v=01f34227"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "displayMath": [["$$", "$$"], ["\\[", "\\]"]]}, "TeX": {"Macros": {"coloneqq": "\\mathrel{\\mathpalette\\coloneqq@{}}", "parallel": "\\parallel"}}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/custom.js?v=35170ed4"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Discussion" href="7_discussions.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
  <div role="main">
    
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="sketching-a-classical-thermodynamic-theory-of-information-for-mri-reconstructions">
<h1>Sketching a Classical Thermodynamic Theory of Information for MRI Reconstructions<a class="headerlink" href="#sketching-a-classical-thermodynamic-theory-of-information-for-mri-reconstructions" title="Link to this heading">¶</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">¶</a></h2>
<p>The present section is a non-formal essay to sketch some basic features of what could be a
thermodynamical theory of MRI reconstruction. Our idea is to convince the reader that a
general picture may be sketched in the future,
which includes MRI reconstructions, an entropy notion, computers, electrical power,
reconstruction time, image quality gain, and artificial intelligence.
Placing Monalisa in that picture allows in particular to understand the intuition that
motivated the design of our toolbox: to efficiently consume a
maximum amount of energy with a high performance computer (HPC).</p>
<p>What we do in fact in this text is an analogy between a heat engine and a computer.
In particular, we find a way to describe a computer performing an MRI reconstruction
in the same way that a heat engine that compresses an ideal gaz in a cylinder. We try
to describe an iterative reconstruction as a machine that compresses the space of the
dynamical variables of the reconstruction, and thus lowering the entropy of the computer
memory in the same way that a each engine can lower the entropy of an ideal gas.</p>
<p>As the reader will notice, this discussion can be applied to any iterative
algorithm that solves an inverse problem. The MRI reconstruction process is used here
as a representent example for any iterative inverse-problem solving process. Given the generality
of the statments exposed in this discussion, we can consider it as an attempt to
formulate a classical (non-quantum) physical theory of information. In the discussion section,
we will infact make some connection with the principle of Landauer, which makes the bridge
petween physics and information theory by providing an equivalence between energy and information.</p>
</section>
<section id="iterative-reconstructions-and-phase-space">
<h2>Iterative Reconstructions and Phase Space<a class="headerlink" href="#iterative-reconstructions-and-phase-space" title="Link to this heading">¶</a></h2>
<p>Before anything, we would like to indicate that the MRI reconstructions under consideration
here are iterative reconstructions. These reconstructions appeared historically as iterative
algorithms to find a minimizer of the non-constrained optimization problem</p>
<div class="math notranslate nohighlight">
\[x^\# \in \underset{x \in X}{argmin} \lVert {FC x - y} \rVert ^2_{Y, 2} + \frac{\lambda}{2} R(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(FC\)</span> is the linear model of the reconstruction, <span class="math notranslate nohighlight">\(X\)</span> is the set of MRI images,
<span class="math notranslate nohighlight">\(x\)</span> is the candidate MRI image, <span class="math notranslate nohighlight">\(Y\)</span> is the set of MRI data,
<span class="math notranslate nohighlight">\(y\)</span> is the measured MRI data, <span class="math notranslate nohighlight">\(\lambda\)</span> is the regularization parameter,
and <span class="math notranslate nohighlight">\(R\)</span> is a regulariztion function with some nice properties (typically is <span class="math notranslate nohighlight">\(R\)</span> chosen to be proper,
closed, convex). The objective function of the above optimization problem is made of</p>
<blockquote>
<div><ul class="simple">
<li><p>a data-fidelity term, which is small when the modeled data <span class="math notranslate nohighlight">\(FCx\)</span> is close to the measured data <span class="math notranslate nohighlight">\(y\)</span>,</p></li>
<li><p>a regularization term, which is small when the image <span class="math notranslate nohighlight">\(x\)</span> is close to satisfying some prior-knowledge.</p></li>
</ul>
</div></blockquote>
<p>In this formalism, the choice of a regularization function implements a choice of prior-knowledge.</p>
<p>This <em>argmin</em> problem is the conventional modern formulation of the general MRI reconstruction problem.
For many choices of regularization function, the reconstruction problem has some minimizers and
there exists some iterative algorithm that converge to one of the minimizers,
which depends usually on the initial image guess.
Those algorithms applied to the MRI reconstruction problems are the conventional
iterative reconstruction method. In addition to these conventional methods,
some heurisitcs methods are inspired from the conventional ones
but perform some heuristic update at each iteration. These methods converge in some cases but they do not
minimize a given objective function and their convergence is not necessarily guaranteed by any mathematical formalism.
Some examples of such heuristic reconstruction are iterative methods where some updates of the image or other
dynamic variables are done by a statistical model. It is an example of use of artificial inteligence for MRI reconstruction.</p>
<p>Heuristic or not, we consider in the following iterative reconstructions that converges for some given dataset.
From the view point of discrete dynamical system theory, we can summarize an iterative reconstruction as follows.
An iterative reconstruction is given by a map <span class="math notranslate nohighlight">\(\Phi\)</span> from <span class="math notranslate nohighlight">\(X \times Z\)</span> to <span class="math notranslate nohighlight">\(X \times Z\)</span>,
which is parametrized by a list of parameters <em>param</em> and the measured data <span class="math notranslate nohighlight">\(y\)</span>.
Here is <span class="math notranslate nohighlight">\(X\)</span> the vector space of all MRI images of a given size, and <span class="math notranslate nohighlight">\(Z\)</span> is the cartesian product of
all spaces that contain all other dynamical variables that we will write as a single list <span class="math notranslate nohighlight">\(z\)</span>.
We considere that a parameter is a constant, known, controlled number and <span class="math notranslate nohighlight">\(param\)</span> is the list of those.
It includes for example the regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>It holds then</p>
<div class="math notranslate nohighlight">
\[(x^{(c+1)}, z^{(c+1)}) =  \Phi(x^{(c)}, z^{(c)}; y, param) = \Phi^{(c)}(x^{(0)}, z^{(0)}; y, param)\]</div>
<p>Note that we write parameters after the “;” and the dynamical variables before.</p>
<p>We expect from that dynamical system that for any initial value <span class="math notranslate nohighlight">\((x^{(0)}, z^{(0)})\)</span> the sequence
converges to a pair <span class="math notranslate nohighlight">\((x^{(inf)}, z^{(inf)})\)</span>,  which may depend on the initial value. The set
of all limits is the <strong>attractor</strong> of the dynamical system, that we will write <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>.
The stability of each element <em>a</em> of the attractor may then be analyzed by the tool of dynamic system theory.
Note that in practice, the attractor is larger than a single point and the <strong>bassin of attaction</strong> <span class="math notranslate nohighlight">\(\mathcal{B}(a)\)</span>.
of an elemment <em>a</em> of the attractor (the set of initial values that lead to a sequence converging to <em>a</em>)
is also lager than a single point.</p>
<p>For a given objective function to minimize, the art of building an algorithm that finds a minimizer
consists of building a map <span class="math notranslate nohighlight">\(\Phi\)</span> for which the projection of its bassin of attraction on the
space <span class="math notranslate nohighlight">\(X\)</span> co-inside with the set of minimizer of the objective function (or a subset of it).</p>
<p>We further would like to point out that a non-iterative (single step) reconstruction can
be seen as an iterative reconstruction.
For that, we only have to realize that a non-iterative reconstruction is given by a map <span class="math notranslate nohighlight">\(\phi\)</span>
that does not depends on <span class="math notranslate nohighlight">\((x, z)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\Phi(x^{(c)}, z^{(c)}; y, param) = \phi(y, param)\]</div>
<p>In that sense, all reconstruction are iterative and those we called “non-iterative”
converge in a single step since</p>
<div class="math notranslate nohighlight">
\[(x^{(c+1)}, z^{(c+1)}) = \Phi(x^{(c)}, z^{(c)}; y, param) = \phi(y, param) =  \Phi(x^{(0)}, h^{(0)}; y ,  param)\]</div>
<p>Iterative reconstruction guided by (based on, unsing, enhenced by…) artificial inteligence
can be seen as a dynamic system where the implelentation of <span class="math notranslate nohighlight">\(\Phi\)</span> contains some
statistical model. For example, if <span class="math notranslate nohighlight">\(\mathcal{N}\)</span> is a neuronal network trained to predict
some of the dynamical variables from the measured data set and from a database of good quality images,
it can be used to update that dynamical variable as each iteration. We can then see <span class="math notranslate nohighlight">\(\mathcal{N}\)</span>
as a parameter of the map <span class="math notranslate nohighlight">\(\Phi\)</span>:</p>
<div class="math notranslate nohighlight">
\[(x^{(c+1)}, z^{(c+1)}) =  \Phi(x^{(c)}, z^{(c)}; y , \mathcal{N}, param)\]</div>
<p>For futur needs, we define here our <strong>phase space</strong> of MRI reconstruction. For that, we will
get some inpiration from the physics. The spirit of phase space in physics is the
following. The phase space is a set so that each of its element corresponds to exaclty one
of the state that the physical system under consideretion can occupy,
and each of these element carries the complete information about the system occupying that state.
In classical Hamiltonian mechanic for example, if one knows the position in phase space
of a physical system at some time, then everything about the system is known at that
time. In particular, it is then possible to predict all futur states of the system and
find all its past states. In our case of MRI reconstruction, the map <span class="math notranslate nohighlight">\(\Phi\)</span> that
dictates the dynamic may not be invertible. We therefore cannot expect to recover
the past hisory of a position in phase space, but at least its future states.
It makes therefore sense to define our phase space as</p>
<div class="math notranslate nohighlight">
\[\Gamma =  X \times Z\]</div>
<p>The state of our system at a given time (a given iteration) is then gien by a
pair <span class="math notranslate nohighlight">\((x, z)\)</span> and its knowledge is sufficient to predict all future states
by iterating <span class="math notranslate nohighlight">\(\Phi\)</span> on that pair. Note that the attractor <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is
a proper subset of the phase-space <span class="math notranslate nohighlight">\(\Gamma\)</span>. Instead of writing <span class="math notranslate nohighlight">\((x, z)\)</span> we
will also write just <span class="math notranslate nohighlight">\(\omega\)</span>. We will write the initial value as</p>
<div class="math notranslate nohighlight">
\[\omega^{(0)} = (x^{(0)}, z^{(0)})\]</div>
<p>and we we will write the state after <span class="math notranslate nohighlight">\(c\)</span> iterations as</p>
<div class="math notranslate nohighlight">
\[\omega^{(c)} = (x^{(c)}, z^{(c)}) = \Phi^{(c)}(x^{(0)}, z^{(0)}; y, param) = \Phi^{(c)}(\omega^{(0)}; y, param)\]</div>
<p>An iterative reconstruction process can then be described in two steps:</p>
<blockquote>
<div><ul class="simple">
<li><p>to choose an initial guess for the image and the other dynamic variables in a set <span class="math notranslate nohighlight">\(\Omega^{(0)} \subset \Gamma\)</span>. We will call that initial guess <span class="math notranslate nohighlight">\(\omega^{(0)} = (x^{(0)}, z^{(0)}) \in \Omega^{(0)}\)</span>.</p></li>
<li><p>to iterate <span class="math notranslate nohighlight">\(\Phi\)</span> on <span class="math notranslate nohighlight">\(\omega^{(0)}\)</span> until the obtained value <span class="math notranslate nohighlight">\(\omega^{(c)} = \Phi^{(c)}(\omega^{(0)}; y, param)\)</span> is sufficiently close to the attractor <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>.</p></li>
</ul>
</div></blockquote>
<p>The description of the second step is however not appropriate to the thermodynamical description we are going to present.
In order to prepare the rest of the discussion, we need to reformulate those two steps in term of sets and distributions.
For a given subset <span class="math notranslate nohighlight">\(\Omega \subset \Gamma\)</span> we define</p>
<div class="math notranslate nohighlight">
\[\Phi^{(c)}(\Omega; y, param) := \{\Phi^{(c)}(\omega; y, param) | \omega \in \Omega\}\]</div>
<p>Our phase space <span class="math notranslate nohighlight">\(\Gamma\)</span> can be considered as isomorphic to <span class="math notranslate nohighlight">\(\mathbb{C}^n\)</span> for some
positive interger <span class="math notranslate nohighlight">\(n\)</span> in our context of MRI reconstruction.
Topologically, it is thus identical to <span class="math notranslate nohighlight">\(\mathbb{R}^{2n}\)</span>. We will write <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> the <span class="math notranslate nohighlight">\(\sigma\)</span>-algebra
of the Lebesgue measurable sets of <span class="math notranslate nohighlight">\(\Gamma\)</span> so that the pair <span class="math notranslate nohighlight">\((\Gamma, \mathcal{L})\)</span> is a measurable space.
We can then provide this measurable space with any measure <span class="math notranslate nohighlight">\(\nu\)</span> to obtain a measure space.</p>
<p>We assume threrefore that <span class="math notranslate nohighlight">\(\Gamma\)</span> is a measure space with measure <span class="math notranslate nohighlight">\(\nu\)</span> and we assume that any measurable
subset <span class="math notranslate nohighlight">\(\Omega \subset \Gamma\)</span> verifies</p>
<div class="math notranslate nohighlight">
\[\nu \left( \Omega \right) = \int_{\Omega}  d\nu = \int_{\Omega} f_{\nu}(\omega) d\omega\]</div>
<p>where the integral with respect to <span class="math notranslate nohighlight">\(\omega\)</span> is the Lebesgue integral
and <span class="math notranslate nohighlight">\(f_{\nu}\)</span> is the Radon-Nikodym derivative of <span class="math notranslate nohighlight">\(\nu\)</span> with respect to the Lebesgue measure.</p>
<p>In order to build a connection with information theory later, we want to interpret <span class="math notranslate nohighlight">\(\nu\)</span> as a probability
measure. But for that we need to specify a subset <span class="math notranslate nohighlight">\(P \subset \Gamma\)</span> so that</p>
<div class="math notranslate nohighlight">
\[\nu\left(P\right) = 1\]</div>
<p>We considere then all Lebesgue measurable sets which are also subset of <span class="math notranslate nohighlight">\(P\)</span>. They
form a <span class="math notranslate nohighlight">\(\sigma\)</span>-algebra that we will write <span class="math notranslate nohighlight">\(\mathcal{L}_{P}\)</span>. Given a measure <span class="math notranslate nohighlight">\(\nu\)</span>
on <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>, we can normalize it so that the measure by <span class="math notranslate nohighlight">\(\nu\)</span> of <span class="math notranslate nohighlight">\(\mathcal{L}_{P}\)</span>
is 1, so that the tripple <span class="math notranslate nohighlight">\(\left(P, \mathcal{L}_{P}, \nu \right)\)</span> is a probability space
(which is nothing more than a measure space where the measure of the entire space is 1). We will also set the constraint</p>
<div class="math notranslate nohighlight">
\[f_{\nu}(\omega) &gt; 0 \quad \forall \omega \in P\]</div>
<p>in order to avoid some division by 0. There are then mainly two choices of
interest for <span class="math notranslate nohighlight">\(P\)</span> in our discussion. In the first case, we will
set <span class="math notranslate nohighlight">\(P\)</span> equal to <span class="math notranslate nohighlight">\(\Gamma\)</span>, while in the second case, we will
set <span class="math notranslate nohighlight">\(P\)</span> equal to <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span>, as define hereafter.</p>
<p>We will write <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span> the subset of <span class="math notranslate nohighlight">\(\Gamma\)</span> in which the initial value is chosen
and we will set on it the restriction <span class="math notranslate nohighlight">\(\mathcal{A} \subset \Omega^{(0)}\)</span>.</p>
<p>We will write <span class="math notranslate nohighlight">\(\Omega^{(c)}\)</span> the subset of <span class="math notranslate nohighlight">\(\Gamma\)</span> defined by</p>
<div class="math notranslate nohighlight">
\[\Omega^{(c)} := \Phi^{(c)}(\Omega^{(0)}; y, param)\]</div>
<p>It is the set that contains <span class="math notranslate nohighlight">\(\omega^{(c)}\)</span>, whatever the initial value of the reconstruction process is.</p>
<p>Let be <span class="math notranslate nohighlight">\(\mu^{(0)}\)</span> a probability measure on <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span> with probability distribution
function (PDF) <span class="math notranslate nohighlight">\(p_{\mu^{(0)}}\)</span> so that the probability that the random variable associated to
<span class="math notranslate nohighlight">\(\mu^{(0)}\)</span> appears in a set <span class="math notranslate nohighlight">\(\Omega \subset \Omega^{(0)}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\mu^{(0)} \left( \Omega \right) = \int_{\Omega}  d\mu^{(0)} = \int_{\Omega}  p_{\mu^{(0)}}(\omega) d\nu\]</div>
<p>where <span class="math notranslate nohighlight">\(p_{\mu^{(0)}}\)</span> is the Radon-Nikodym derivative of <span class="math notranslate nohighlight">\(\mu^{(0)}\)</span> with respect to <span class="math notranslate nohighlight">\(\nu\)</span>.
It holds in particular</p>
<div class="math notranslate nohighlight">
\[\mu^{(0)} \left( \Omega^{(0)} \right) = 1\]</div>
<p>We then reformulate the two steps above as follows:</p>
<blockquote>
<div><ul class="simple">
<li><p>Instead of chosing an initial guess, we chose a probability measure <span class="math notranslate nohighlight">\(\mu^{(0)}\)</span> on the set <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span> so that <span class="math notranslate nohighlight">\(\mu^{(0)}(\Omega^{(0)}) = 1\)</span> and so that the initial value <span class="math notranslate nohighlight">\(\omega^{(0)}\)</span> is a random variable with PDF equal to <span class="math notranslate nohighlight">\(p_{\mu^{(0)}}\)</span>.</p></li>
<li><p>We describe then the iteration process as a contraction of <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span> by iterating on it the map <span class="math notranslate nohighlight">\(\Phi\)</span> until <span class="math notranslate nohighlight">\(\Phi^{(c)}(\Omega^{(0)}; y, param)\)</span> becomes sufficiently close to <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>.</p></li>
</ul>
</div></blockquote>
<p>Note that function <span class="math notranslate nohighlight">\(p_{\mu^{(0)}}\)</span> can be extended over <span class="math notranslate nohighlight">\(\Gamma\)</span>  be setting it equal to <span class="math notranslate nohighlight">\(0\)</span> outside <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span>.
The following figure summarizes the situation.</p>
<a class="reference internal image-reference" href="_images/information.png"><img alt="information" class="align-center" src="_images/information.png" style="width: 40%;" />
</a>
<p>This description in term of sets and probability disctributions makes abstraction
of the particular image guess and of the reconstructed image. It can be
considered as a mathematical description of the reconstruction of all possible MRI
images in parallel, that would be obtained by chosing all initial guess
in <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span> in parallel, with a given “density of choice” <span class="math notranslate nohighlight">\(\mu^{(0)}\)</span>.</p>
</section>
<section id="the-heat-engine-and-the-computer">
<h2>The Heat Engine and the Computer<a class="headerlink" href="#the-heat-engine-and-the-computer" title="Link to this heading">¶</a></h2>
<p>Here are a few empirically facts. If the reader does not agree with them,
just consider that they are assumptions. We assume furthermore that the iterative reconstruction
in question is correctly implemented.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Some iterative reconstructions are converging for some given data.</p></li>
<li><p>Given some data and an iterative reconstruction that verifies points 1., the image quality along iterations improves then monotonically, at least in average in some temporal window.</p></li>
<li><p>Each iteration of an iterative reconstruction consumes electric power and time, the product of both (or time integral of power) being the energy consumed by that iteration.</p></li>
<li><p>An image is physically a certain state of the memory of the computer. A reconstruction process, in particular a converging iterative reconstruction, is a process that changes the state of the computer memory until the resulting image do not longer significantly changes.</p></li>
<li><p>During an iterative reconstruction process, if the reconstructed image improves and converges (at least in average in some temporal window), the computer absorbs electrical energy, a part of that energy serves to set its memory in a certain state, and most of the absorbed energy is released in the environment as heat.</p></li>
<li><p>A reconstructed image of good quality is an image that models the measured data reasonably well (relative to a given model), and which satisfies some prior knowledge reasonably well. Both criteria result in a low value of the objective function if that function exist.</p></li>
<li><p>An image of good quality is a certain state of the computer memory that has very little chance to be found by chance alone, for example by a random search for a good image.</p></li>
</ol>
</div></blockquote>
<p>It is not the intention of the author to build some axioms of a mathematical theory.
The empirical facts above are in fact redundant to some extends, but we don’t
really care. We just want to build an intuition for a thermodynamic theory of MRI reconstruction.</p>
<p>Since the reconstruction we want to consider has to verify point 2 mentioned above, we do the following definition:</p>
<blockquote>
<div><p>An iterative MRI reconstruction (an implementation) is <strong>well-posed</strong> for some given measured data if</p>
<blockquote>
<div><ul class="simple">
<li><p>The reconstruction is converging for the given data,</p></li>
<li><p>After a finit number of iterations, the image quality along iterations improves monotonically, at least in average (at least in average in some temporal window).</p></li>
</ul>
</div></blockquote>
</div></blockquote>
<p>From the facts listed above, it is intuitively clear that for a well-posed MRI reconstruction (for some given data),
<strong>energy</strong> must be consumed at every iteration that performs an <strong>image quality gain</strong> <em>(IQG)</em>.
The reverse does however not need to be true: more energy consumption
does not need to lead to a gain of image quality, since energy can be directly dissipated into heat.
A notion of <strong>efficiency</strong> is therefore missing and there is no obious definition for it.
The only thing we can say is, that efficiency should to be defined in such a way that it expresses an <em>IQG</em>
related in some way to the energy consumed for that gain. As a consequence, the definition of efficiency must be
closely related to the definition of <em>IQG</em> (and by extension to image quality). We could be tempted to
say that the notion of <em>IQG</em> is the analog of the <em>work</em> in the thermodynamic of heat engines. Following that intuition,
the author tried the following analogy between a heat engine and a computer (engine).</p>
<p>Work is the useful thing that a heat engine give to some part of the unisvers that we will call the <strong>work environment</strong>.
The heat engine performs some work in the work environment by transferring heat from a hot to a cold reservoir.
The heat engine and the working environment are two subsytems and the hot reservoir, cold reservoir and the <em>rest of the universe</em>
are three other subsystems. Their union being the universe (the total system).</p>
<blockquote>
<div><a class="reference internal image-reference" href="_images/heat_engine_1.png"><img alt="heat_engine_1" class="align-center" src="_images/heat_engine_1.png" style="width: 90%;" />
</a>
</div></blockquote>
<p>The heat engine operates in a cyclic way so that its state is the same at the beginning of each new cycle.
In contrast, the states of the work environment, the <em>rest of the universe</em> and the heat reservoirs
can evolve along the cycles. The goal of a heat engine
is in fact to transform the work environment, else the engine would be useless. The transformation of the work
environment often translates in a lowering of its <strong>entropy</strong>, while the entropy of
the <em>rest of the universe</em> together with the heat reservoirs is increasing. The transformation is reversible exactly if
the entropy of the universe (total system) remains constant during that transformation.
If the transformation is irreversible, the entropy of the universe increases, even if entropy of the work environment decreases.
Since the entropy is a function of state, the entropy of the heat engine is the same at the beginning (and end) of each cycle.</p>
<p>For a the coming comparison between a computer and a heat engine, we would like to focus on the special case
described in the following figure.</p>
<blockquote>
<div><a class="reference internal image-reference" href="_images/heat_engine_2.png"><img alt="heat_engine_2" class="align-center" src="_images/heat_engine_2.png" style="width: 90%;" />
</a>
</div></blockquote>
<p>It represents a heat engine that gives energy to a working environment (<em>WE</em>) in the form of a mechanical work amount <em>W</em>.
This work is used to compress an ideal gaz in a cylinder in thermal contact with the cold reservoir at temperatur <span class="math notranslate nohighlight">\(T_C\)</span>.
In order to be able to evaluate entropy changes, we admit that no irreversible loss of energy happens.
This means that the heat engine is an ideal (reversible) heat engine, which is called a <em>Carnot engine</em>. It has therefore
maximal efficiency. We also have to assume that the gaz compression is isothermal, which means
that the movement has to be sufficiently slow as garantied by the coupling of the small and large wheels.
We admit that there is a good isolation between the <em>rest of the universe</em> and to two subsystem implied in the process,
which are the heat engine and the WE. A flow of energy travels through the subsystem made of the pair <em>heat-engine + WE</em>.
At each cycle of the engine, a heat amount</p>
<div class="math notranslate nohighlight">
\[E_{in} = \lvert \Delta Q_H \rvert\]</div>
<p>enters that subsytem and a heat amount</p>
<div class="math notranslate nohighlight">
\[E_{out} = \lvert \Delta Q_C \rvert + \lvert \Delta Q_{WE} \rvert\]</div>
<p>leaves that sub system. Since the temperature of the gaz in the <em>WE</em> do not changes, its internal energy do not
change as well. That means that the work <span class="math notranslate nohighlight">\(\Delta W\)</span> is equal to the expelled heat amount <span class="math notranslate nohighlight">\(\lvert \Delta Q_{WE} \rvert\)</span>.
The conservation of energy reads thus:</p>
<div class="math notranslate nohighlight">
\[\lvert \Delta Q_H \rvert = \lvert \Delta Q_C \rvert + \lvert \Delta Q_{WE} \rvert\]</div>
<p>The volume of the ideal gaz is decreased by an ammount <span class="math notranslate nohighlight">\(\lvert \Delta V \rvert\)</span> at each cycle.
We will write <span class="math notranslate nohighlight">\(V &gt; 0\)</span> the volume of the ideal gaz at the current cycle.
The change of entropy <span class="math notranslate nohighlight">\(\lvert \Delta S_{WE} \rvert\)</span> is therefore negative and given by</p>
<div class="math notranslate nohighlight">
\[\Delta S_{WE} = n \cdot R \cdot log(1-\lvert \Delta V \rvert/V) &lt; 0\]</div>
<p>where <em>n</em> in the chemical amount of ideal gaz and <em>R</em> is the ideal gaz constant.</p>
<p>During one cycle, the hot reservoir experiences a drope of entropy by an ammount</p>
<div class="math notranslate nohighlight">
\[\Delta S_{H} = -\frac{\lvert \Delta Q_H \rvert}{T_H}\]</div>
<p>while the cold reservoir experiences a grow of entropy by an ammount</p>
<div class="math notranslate nohighlight">
\[\Delta S_{C} = +\frac{\lvert \Delta Q_C \rvert}{T_C}\]</div>
<p>Since the engine comes back to the same state after every cycle and since entropy
is a function of state, there is no change of entropy in the engine after each cycle.
Assuming the process to be reversible, the total entropy is conserved:</p>
<div class="math notranslate nohighlight">
\[\Delta S_{C} + \Delta S_{H} + \Delta S_{WE} = 0\]</div>
<p>If the process is now irreversible (like any realistic, non-ideal process), the entropy drope in the ideal gaz will
still be the same since the entropy is a function of state, but the heat exchanges will be different and
this will lead to a positive entropy grow of the universe (the total system) by the second law of thermodynamic,
even if entropy was localy decreased in the ideal gaz:</p>
<div class="math notranslate nohighlight">
\[\Delta S_{C} + \Delta S_{H} + \Delta S_{WE} &gt; 0\]</div>
<p>This scheme of producing an energy flow through a system in order to drain out some of its entropy
(a side effect being an entropy grow of the universe) is a general scheme encountered everywhere
in engineering and nature. Plants and animal do that all the time. We eat energy to produce
mechanical work such as moving from a place to the other, but a large part of the energy we eat
is expelled as thermal radiation associated to a drope of our entropy. In fact, our body continuously
experiences injuries because chance unbuild things more often that it builds it. Those injuries are structural
changes that have a high probability to happen by chance alone and wich correspond to an increase of entropy of
our body. Because of injuries, the entropy of our body tends to increase. In order to survive,
we have to consume energy to continuously put our body back to order i.e. to a state that has very little
chance to be reached by chance a lone, that is, a state a low entropy. Repairing our body implies thus to
consume energy to lower our entropy back to an organized state and that implies to expell an
associated amount of heat by radiation. This scheme is so universal that we will now try
to apply it to computers in order to build an analogy with the eat engine. We will try that way to deduce
a definition of <em>image quality gain (IQG)</em> and <em>efficiency</em> in the contet ot MRI reconstruction.</p>
<p>In the case of MRI reconstruction, the <em>IQG</em> is the useful thing that the computer
produces by absorbing electrical energy and expelling it as heat in the cooling system of the HPC,
which may be interpreted as the cold reservoir. In order to make an analogy between the computer and
the heat engine, we define the following parts of the universe:</p>
<blockquote>
<div><ul class="simple">
<li><p>the <strong>electric power supply system</strong> <em>(PS)</em>, which transfers energy to the computer,</p></li>
<li><p>the <strong>computer</strong> <em>(Comp)</em>, with the computational units and including the part of memory that contains the program, but without the part of memory that contains the dynamic variables of the reconstruction process,</p></li>
<li><p>the part of memory that contains the dynamic variable of the reconstruction process, that we will call the <strong>dynamic memory</strong> (<em>DM</em>).</p></li>
<li><p>the <strong>cooling system</strong> <em>(C)</em> of the computer.</p></li>
<li><p>the <strong>rest of the universe</strong>, which also absorb parts of the heat released by the computer.</p></li>
</ul>
</div></blockquote>
<p>Note that the union of these five parts is the universe.</p>
<blockquote>
<div><a class="reference internal image-reference" href="_images/computer_engine_1.png"><img alt="heat_engine_1" class="align-center" src="_images/computer_engine_1.png" style="width: 90%;" />
</a>
</div></blockquote>
<p>We propose here to consider the computer as an engine and to interpret one iteration of the reconstruction
process as one cycle of the engine. In fact, at the begining of each iteration, the state of the computer
is the same since we consider all changing (dynamic) variables to be in the <em>dynamic memory</em>,
which is the analog of the work environment of the heat engine. The energy given to the computer is almost completely
dissipated into heat transmitted to the cooling system at temperature <span class="math notranslate nohighlight">\(T_C\)</span>. We neglect transmition of heat given to
the <em>rest of the universe</em> because it should be much smaller. Also, there are some
electro-magnetic radiations emited from to the computer to the <em>rest of the universe</em> and some eletrostatic energy
that is stored in the memory, since writing information in it implies to set a certain configuration of charges
with the associated electro-static energy. These two energy amounts are however so small as compared to the energy
dissipated in the cooling system that we will nelglect them. As a consequence of energy conservation we will therefore write
for one cycle</p>
<div class="math notranslate nohighlight">
\[\Delta E_{in} = \lvert \Delta Q_C \rvert\]</div>
<p>That means that all the energy entering the computer is dissipated as heat in the cooling system.
Following the intuition that this flow of energy drains out some (thermodynamical) entropy from the
dynamic memory (<em>DM</em>) as it brings it in a state that can harldy be reached by chance alone,
we expect that a negative entropy change <span class="math notranslate nohighlight">\(-\lvert \Delta S_{DM} \rvert\)</span> is produced in the <em>DM</em> during one
cycle (one iteration) of the MRI reconstruction process. We know that it holds</p>
<div class="math notranslate nohighlight">
\[\Delta S_{DM} \geq \frac{\Delta Q_C}{T_C}\]</div>
<p>where equality holds for a reversible process. But the quantities <span class="math notranslate nohighlight">\(\Delta S_{DM}\)</span> and <span class="math notranslate nohighlight">\(\Delta Q_C\)</span> are signed in that expression.
Assuming <span class="math notranslate nohighlight">\(\Delta S_{DM}\)</span> to be negative, we deduce</p>
<div class="math notranslate nohighlight">
\[\lvert \Delta S_{DM} \rvert \leq \frac{\lvert \Delta Q_C \rvert}{T_C}\]</div>
<p>Since the computer is in the same state at the beginning of each iteration, it experiences no entropy change
between each start of a new iteration. The entropy change in the system <em>computer + DM</em> is therefore
to be attributed to the entropy change in the <em>DM</em> only. The previous inequation means that for an entropy drope
of magnitude <span class="math notranslate nohighlight">\(\lvert \Delta S_{DM} \rvert\)</span> in the <em>DM</em>, there must be a heat amount of magnitude at least
<span class="math notranslate nohighlight">\(T_C \lvert \Delta S_{DM} \rvert\)</span> expelled to the cooling system. We will write <span class="math notranslate nohighlight">\(E^{tot}\)</span> the total amount
of energy given to the computer for the reconstruction and <span class="math notranslate nohighlight">\(\lvert \Delta S_{DM}^{tot} \rvert\)</span> the magnitude
of the total entropy drope in the <em>DM</em> during reconstruction. It follows from the previous equation,
from our formula for energy conservation and from the fact the temperature of the cooling system is constant, that</p>
<div class="math notranslate nohighlight">
\[\lvert \Delta S_{DM}^{tot} \rvert \leq \frac{E^{tot}}{T_C}\]</div>
<p>If we express <span class="math notranslate nohighlight">\(E^{tot}\)</span> as the multiplication of the input electric power <span class="math notranslate nohighlight">\(P\)</span> and the total
reconstruction time <span class="math notranslate nohighlight">\(\Delta t^{tot}\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[\lvert \Delta S_{DM}^{tot} \rvert \leq \frac{P \Delta t^{tot}}{T_C}\]</div>
<p>If we can find a way to establish the magnitude of the total entropy drope in the <em>DM</em> associated
to a desired image quality gain (<em>IQG</em>), for a known electric power, we could then deduce a minimal
reconstruction time for the desired <em>IQG</em>. We however still face the difficulty to define <em>IQG</em>.
In addition, we are unable to continue the analogy between the computer and the heat engine
because we are for the moment unable to define what the computer is transmitting to the <em>DM</em>,
as pointed out by the quotation mark in the last figure. The reason is that the computer
performs no mechanical work and we have to find a replacement for work in order to continue the
analogy. We need now to invent someting.</p>
<p>We propose to solve our difficulties by the following heuristic (actually quite esotherique) construction,
because it is the best we have to the moment. Instead of considerng that the computer interacts
with the dynamic memory, we consider that nature is <em>as if</em> the computer was interacting with the
phase space. The variables stored in the <em>DM</em> represent one state in the phase space,
but since it could be any, the computer behaves in a way that would do the job for any state
in the phase space. We considere therefore that it is a reasonable argument to say that the behaviour of the
computer is related phase space and not related one particular representent.
The computer behaves as if it was reconstructing many MRI images at the same time. Instead of
discussing endlessly how realistic or not that argumentation is, we propose here one implementation
of that idea and we will pragmatically try to see what are the implications.</p>
<p>In analogy to the isothermal compression of an ideal gas, we will consider that the computer
is compressing a portion <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span> of phase space by iterating the map <span class="math notranslate nohighlight">\(\Phi\)</span> that dictates
the evolution of the iterative MRI reconstruction algoritme. We chose <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span> to be the
region of phase space where there is a non-zero probabiliy that our initial value <span class="math notranslate nohighlight">\(\omega^{(0)}\)</span>
is chosen. For convenience, we will like to think of <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span> as a proper, closed, convex set.
We recall that it contains the attractor <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> of the dynamical system. We define the set</p>
<div class="math notranslate nohighlight">
\[\Omega^{(c)} := \Phi^{(c)}(\Omega^{(0)}; y, param)\]</div>
<p>We imagine that <span class="math notranslate nohighlight">\(\Omega^{(c)}\)</span> <em>is</em> the set <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span> compressed by <span class="math notranslate nohighlight">\(\Phi\)</span> after
<span class="math notranslate nohighlight">\((c)\)</span> iterations. We imagine that <span class="math notranslate nohighlight">\(\Omega^{(c)}\)</span> contains an ideal <em>phase space gas</em> and
that at each iteration, a part of the energy given to the computer is transformed in a kind of
<em>informatic work W</em> to compresse that phase space gas. The situation is described in the following figure.</p>
<blockquote>
<div><a class="reference internal image-reference" href="_images/computer_engine_2.png"><img alt="heat_engine_2" class="align-center" src="_images/computer_engine_2.png" style="width: 90%;" />
</a>
</div></blockquote>
<p>We will imagine that any conected proper subsest <span class="math notranslate nohighlight">\(\Omega\)</span> of phase space contains
a certain amount of our “phase space ideal gas”. Inpired by the equation that describs
an ideal gas with constant temperature, we set</p>
<div class="math notranslate nohighlight">
\[P \cdot V = K_{\Gamma}\]</div>
<p>where <em>P</em> is the pressure of our phase space gas, <em>V</em> is its volume given by the measure <span class="math notranslate nohighlight">\(\nu\)</span> as</p>
<div class="math notranslate nohighlight">
\[V = \nu \left(\Omega \right)\]</div>
<p>and <span class="math notranslate nohighlight">\(K_{\Gamma}\)</span> is the ideal gas constant of our phase space gas for a given temperature.
It follows that</p>
<div class="math notranslate nohighlight">
\[P \cdot dV = K_{\Gamma} \cdot \frac{dV}{V}\]</div>
<p>We deduce that the work <span class="math notranslate nohighlight">\(\Delta W\)</span> needed to compress <span class="math notranslate nohighlight">\(\Omega\)</span> to a smaller subset is <span class="math notranslate nohighlight">\(\Omega'\)</span> is</p>
<div class="math notranslate nohighlight">
\[\Delta W = - K_{\Gamma} \int_{\nu \left(\Omega \right)}^{\nu \left(\Omega' \right)} \frac{dV}{V} = - K_{\Gamma} \cdot log \left( \frac{\nu(\Omega')}{\nu(\Omega)} \right)\]</div>
<p>We will now label some quantities with the super-script <span class="math notranslate nohighlight">\((c, c+1)\)</span> to indicate that the quantity in question
is associated to the iteration numner <span class="math notranslate nohighlight">\((c)\)</span>, which performs the transition from state <span class="math notranslate nohighlight">\((c)\)</span> to state <span class="math notranslate nohighlight">\((c+1)\)</span>.
We will also label a quantity with super-script <span class="math notranslate nohighlight">\((c)\)</span> in order to indicate that this quantity is associated to the transition
from the initial state to the the state number <span class="math notranslate nohighlight">\((c)\)</span>.</p>
<p>For the comming comparison with information theory in the next subsection,
we define the information gain associated the trasnsition
from <span class="math notranslate nohighlight">\(\Omega^{(c)}\)</span> to <span class="math notranslate nohighlight">\(\Omega^{(c+1)}\)</span> as</p>
<div class="math notranslate nohighlight">
\[\Delta I^{(c, c+1)} := - log \left( \frac{\nu(\Omega^{(c+1)})}{\nu(\Omega^{(c)})} \right)\]</div>
<p>We define as well the gain of information associated to all iterations until (and with) iteration number <span class="math notranslate nohighlight">\((c)\)</span> as</p>
<div class="math notranslate nohighlight">
\[\Delta I^{(c)} := \Delta I^{(0, 1)} + ... +\Delta I^{(c-1, c)}\]</div>
<p>it follows</p>
<div class="math notranslate nohighlight">
\[\Delta I^{(c)} = - \left( log \left( \frac{\nu(\Omega^{(1)})}{\nu(\Omega^{(0)})} \right) + ... + log \left( \frac{\nu(\Omega^{(c)})}{\nu(\Omega^{(c-1)})} \right) \right) = - log \left( \frac{\nu(\Omega^{(c)})}{\nu(\Omega^{(0)})} \right)\]</div>
<p>We get then a relation between physical work (in Joule <em>J</em>) and information given by</p>
<div class="math notranslate nohighlight">
\[\Delta W^{(c, c+1)} = K_{\Gamma} \cdot \Delta I^{(c, c+1)}\]</div>
<p>for iteration number <span class="math notranslate nohighlight">\({(c+1)}\)</span> or alternatively</p>
<div class="math notranslate nohighlight">
\[\Delta W^{(c)} = K_{\Gamma} \cdot \Delta I^{(c)} \quad (E1)\]</div>
<p>for all iteration until (and with) iteration number <span class="math notranslate nohighlight">\({(c+1)}\)</span>.
It follows in particular from these last two equations that,
whatever the unit of information is, the constant <span class="math notranslate nohighlight">\(K_{\Gamma}\)</span> must
have the unit <em>J/[Unit of Information]</em>. We are now able to define
a notion of <em>efficiency</em> <span class="math notranslate nohighlight">\(\eta^{(c, c+1)}\)</span> as the ratio of the input energy
<span class="math notranslate nohighlight">\(\Delta E_{in}^{(c, c+1)}\)</span> (during one cycle) and the work performed
on the phase space <span class="math notranslate nohighlight">\(\Delta W^{(c, c+1)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\eta^{(c, c+1)} := \frac{\Delta W^{(c, c+1)}}{E_{in}^{(c, c+1)}} =  K_{\Gamma} \cdot \frac{\Delta I^{(c, c+1)}}{E_{in}^{(c, c+1)}}\]</div>
<p>What we mean here is that at each cycle, an energy amount <span class="math notranslate nohighlight">\(\Delta E_{in}^{(c, c+1)}\)</span>
is given to the computer, an amount <span class="math notranslate nohighlight">\(\Delta E_{in}^{(c, c+1)} - \Delta W^{(c, c+1)}\)</span> is dissipated
to the cooling sytem by the computation at temperature <span class="math notranslate nohighlight">\(T_C\)</span>, and another
amount <span class="math notranslate nohighlight">\(\Delta W^{(c, c+1)}\)</span> is given as work to the phase space and then also dissipated
to the cooling system as a heat amount <span class="math notranslate nohighlight">\(\lvert Q_{DM}^{(c, c+1)} \rvert\)</span> at
temperature <span class="math notranslate nohighlight">\(T_C\)</span>. It holds thus</p>
<div class="math notranslate nohighlight">
\[\lvert \Delta Q_{DM}^{(c, c+1)} \rvert = \Delta W^{(c, c+1)}\]</div>
<p>We will name <span class="math notranslate nohighlight">\(\lvert \Delta Q_{Comp}^{(c, c+1)} \rvert\)</span> the heat amount dissipated by the
computation directly to the cooling system. This is the part of the energy that is not
“transmited” to the phase space. The conservation of energy can then be rewritten as</p>
<div class="math notranslate nohighlight">
\[\Delta E_{in}^{(c, c+1)} = \lvert \Delta Q_{Comp}^{(c, c+1)} \rvert + \lvert \Delta Q_{DM}^{(c, c+1)} \rvert\]</div>
<p>Of course, the phase space is a mathematical, non-physical object and
the <em>work given to phase space</em> is a symbolic language. What we try to do is an
intelectual effort that consists in admiting that nature behaves <em>as if</em> the
computer was in fact transmiting work to the phase space.</p>
<p>In order to complete the picture and for a comparison with information theory in the next sub-section,
we choose to identify the entropy drope in the <em>DM</em> during iteration number <span class="math notranslate nohighlight">\((c+1)\)</span> as
the entropy drope in the associated phase space ideal gas:</p>
<div class="math notranslate nohighlight">
\[\Delta S^{(c, c+1)}_{DM} = \frac{K_{\Gamma}}{T_C} \cdot log \left( \frac{\nu(\Omega^{(c+1)})}{\nu(\Omega^{(c)})} \right)\]</div>
<p>The total entropy drope due to all iterations until (and with) iteration number <span class="math notranslate nohighlight">\((c)\)</span> is therefore</p>
<div class="math notranslate nohighlight">
\[\Delta S^{(c)}_{DM} = \Delta S^{(0, 1)}_{DM} + ... + \Delta S^{(c-1, c)}_{DM}\]</div>
<p>and thus</p>
<div class="math notranslate nohighlight">
\[\Delta S^{(c)}_{DM} = \frac{K_{\Gamma}}{T_C} \left(log \left( \frac{\nu(\Omega^{(1)})}{\nu(\Omega^{(0)})} \right) + ... + log \left( \frac{\nu(\Omega^{(c)})}{\nu(\Omega^{(c-1)})} \right)\right) = \frac{K_{\Gamma}}{T_C} log \left( \frac{\nu(\Omega^{(c)})}{\nu(\Omega^{(0)})} \right)\]</div>
<p>It follows</p>
<div class="math notranslate nohighlight">
\[\Delta S^{(c)}_{DM} = - \frac{K_{\Gamma}}{T_C} \Delta I^{(c)}\]</div>
<p>We note at that point that it makes sense to consider the information <span class="math notranslate nohighlight">\(I\)</span> as a state variable of the <em>DM</em> which should not depends on
the temperature of the cooling system <span class="math notranslate nohighlight">\(T_C\)</span>. Moreover, for consistency with thermodynamic, the entropy in <em>DM</em> has to be a state variable too and should
therefore not depends on <span class="math notranslate nohighlight">\(T_C\)</span>. The last equation is therefore paradoxical. To resolve that paradox, we will assign the temperature <span class="math notranslate nohighlight">\(T_C\)</span>
to our <em>phase space ideal gas</em>. This implies that <span class="math notranslate nohighlight">\(K_{\Gamma}\)</span> can be expressed by the multiplication of <span class="math notranslate nohighlight">\(T_C\)</span> and another constant
that we will write <span class="math notranslate nohighlight">\(k_{\Gamma}\)</span>:</p>
<div class="math notranslate nohighlight">
\[K_{\Gamma} = T_C \cdot k_{\Gamma}\]</div>
<p>It follows that</p>
<div class="math notranslate nohighlight">
\[\Delta S^{(c)}_{DM} = - k_{\Gamma} \cdot \Delta I^{(c)} = k_{\Gamma} log \left( \frac{\nu(\Omega^{(c)})}{\nu(\Omega^{(0)})} \right) \quad (E2)\]</div>
<p>From our equivalence between energy and information (E1), it follows</p>
<div class="math notranslate nohighlight">
\[\Delta S^{(c, c+1)}_{DM} = - \frac{\Delta W^{(c, c+1)}}{T_C} = \frac{\Delta Q_{DM}^{(c, c+1)}}{T_C}\]</div>
<p>This is consistent with a reversible isothermal compression of an ideal gas, as assumed.
If the process is not reversible (as any realistic process) we expect the inequation</p>
<div class="math notranslate nohighlight">
\[\Delta S^{(c, c+1)}_{DM} \geq \frac{\Delta Q_{DM}^{(c, c+1)}}{T_C}  \quad (E3)\]</div>
<p>We can now make the sum of all entropy changes to obtain the change of the total entropy
during one iteration in a realistic (non-reversible). The cooling system experiences an entropy grow</p>
<div class="math notranslate nohighlight">
\[\Delta S^{(c, c+1)}_{C} \geq \frac{\lvert \Delta Q^{(c, c+1)}_{Comp} \rvert }{T_C} + \frac{\lvert \Delta Q^{(c, c+1)}_{DM} \rvert }{T_C}\]</div>
<p>while the entropy change of the computer over one iteration is zero and the entropy change in the <em>DM</em> is</p>
<div class="math notranslate nohighlight">
\[\Delta S^{(c, c+1)}_{DM} \geq \frac{-\lvert \Delta Q^{(c, c+1)}_{DM} \rvert }{T_C}\]</div>
<p>The total entropy change over one iteration is therefore</p>
<div class="math notranslate nohighlight">
\[\Delta S^{(c, c+1)}_{tot} \geq \frac{\lvert \Delta Q^{(c, c+1)}_{Comp} \rvert }{T_C} + \frac{\lvert \Delta Q^{(c, c+1)}_{DM} \rvert }{T_C}  + \frac{-\lvert \Delta Q^{(c, c+1)}_{DM} \rvert }{T_C} = \frac{\lvert \Delta Q^{(c, c+1)}_{Comp} \rvert }{T_C} &gt; 0\]</div>
<p>If we admit that the <em>DM</em> experiences an entropy drope of
magnitude <span class="math notranslate nohighlight">\(\lvert \Delta S^{(c, c+1)}_{DM} \rvert\)</span> during one
iteration. We deduce from (E3) that</p>
<div class="math notranslate nohighlight">
\[\lvert \Delta S^{(c, c+1)}_{DM} \rvert \leq \frac{\lvert \Delta Q_{DM}^{(c, c+1)} \rvert}{T_C} = \frac{\Delta W^{(c, c+1)}}{T_C} = \frac{\eta^{(c, c+1)} \cdot E_{in}^{(c, c+1)}}{T_C}\]</div>
<p>If the efficiency is constantly equal to a number <span class="math notranslate nohighlight">\(\eta\)</span>, summing up all contribution of the entire reconstruction duration leads</p>
<div class="math notranslate nohighlight">
\[\lvert \Delta S^{tot}_{DM} \rvert \leq \frac{\eta \cdot E_{in}^{tot}}{T_C} = \eta \frac{ P \cdot \Delta t^{tot}}{T_C}\]</div>
<p>which is a more severe constraint on the entropy drope of the <em>DM</em> as compared to the one we got earlier. It follows in
particular that</p>
<div class="math notranslate nohighlight">
\[\Delta I^{tot} \leq  \frac{\eta}{k_{\Gamma}} \frac{ P \cdot \Delta t^{tot}}{T_C} \quad (E4)\]</div>
<p>This inequation is the curiosity of our theory because of its temperature dependency.
We will come back to it in the discussion section.</p>
<p>We also note that, the definition of <em>IQG</em> we were hoping for, did not appear, although we
have defined all basic thermodynamical quantity that where needed for an analogy between
the computer and the heat engine. We will also come back to that point in the
discussion section.</p>
</section>
<section id="connections-with-the-theory-of-information">
<h2>Connections with the Theory of Information<a class="headerlink" href="#connections-with-the-theory-of-information" title="Link to this heading">¶</a></h2>
<p>In the previous subsection, we introduced some relation between the physical energy <em>E</em>
and the thermodynamical entropy <em>S</em> as well as a notion of information <em>I</em> with some
relation to <em>E</em> and <em>S</em>.</p>
<p>In this section, we will introduce some relations that relates the
thermodynamical entropy <em>S</em> to the information theoretical entropy <em>H</em> as well as to
the relative entropy <span class="math notranslate nohighlight">\(D_{KL}\)</span>, also known as <em>Kullback-Leibler divergence</em>.
Also, we will relate our notion of information <em>I</em> to the
notion of information-theoretical information that we will write <span class="math notranslate nohighlight">\(J\)</span>.</p>
<p><em>Work in Progress</em></p>
</section>
<section id="discussion">
<h2>Discussion<a class="headerlink" href="#discussion" title="Link to this heading">¶</a></h2>
<p>By defining the total useful energy consumed by the reconstruction as</p>
<div class="math notranslate nohighlight">
\[\Delta E^{tot}_{useful} =  \eta \cdot P \cdot \Delta t^{tot}\]</div>
<p>equation (E4) can be rewritten as</p>
<div class="math notranslate nohighlight">
\[k_{\Gamma} \ T_C \  \Delta I^{tot} \leq   \Delta E^{tot}_{useful} \quad (E5)\]</div>
<p>This equation is very similar to the principle of Landauer, which reads</p>
<div class="math notranslate nohighlight">
\[k_{B} \ T \  log(2) \leq   \Delta E\]</div>
<p>where <span class="math notranslate nohighlight">\(k_{B}\)</span> is the Boltzmann constant, <span class="math notranslate nohighlight">\(T\)</span> is the temperature of the computer
(which can be identified with the temperature of the cooling system <span class="math notranslate nohighlight">\(T_C\)</span>) and <span class="math notranslate nohighlight">\(\Delta E\)</span>
is the practical energy amount that is needed to erase a <em>bit</em> of information. This similarity suggests
that equation (E5) could be seen as an generalisation of Landauer principle in the context of MRI
reconstruction, but we have to stay prudent for the moment. What we were able to show is,
that interpreting the “work” of the computer as a virtual work that contracts a portion of phase
space like an ideal gas leads to a relation between energy and information that is very similar
to the Principle of Landauer.</p>
<p>The entropy of an ideal gas, for a constant number of particles <span class="math notranslate nohighlight">\(N\)</span> and constant temperature, can be expressed up
to a constant as</p>
<div class="math notranslate nohighlight">
\[S = N \ k_B \ log(V) + const.\]</div>
<p>An analogy with our ideal phase space gas and equation (E2) suggests, for the entropy of the dynamic memory, an expression of the form:</p>
<div class="math notranslate nohighlight">
\[S = k_{\Gamma} \  log\left(\frac{\nu \left(\Omega\right) }{\nu \left(P\right)} \right) = k_{\Gamma} \  log\left( \nu \left(\Omega\right) \right)\]</div>
<p>because by our definitions is <span class="math notranslate nohighlight">\(\nu(P)\)</span> equal to <span class="math notranslate nohighlight">\(1\)</span>. Our theory can be considered of a physical assumbtion about the entropy of the
dynamic memory. We have no way to prove that the true (physical) thermodynamic entropy of the dynamic memory is realy given by our expression.
It is a claim we did and that tried to test it by exploring some of the consequences. Interestingly,
this equation for entropy is very similar to the equation of Boltzmann</p>
<div class="math notranslate nohighlight">
\[S = k_B \  log\left( \Omega \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\Omega\)</span> is the area of the surface in phase space occupied by all the possible micro states of a given energy
for the physical system under consideration (it is the “number” of allowed mirco states, if one prefers).
There is in fact an analogy between the set of allowed microstates of a physical system and our phase space:
our set <span class="math notranslate nohighlight">\(\Omega^{(0)}\)</span> is the set of possible states of the dynamic memory and after <span class="math notranslate nohighlight">\(c\)</span> iterations,
the set of possible states of the dynamic memory is the set <span class="math notranslate nohighlight">\(\Omega^{(c)}\)</span>. The numbers
<span class="math notranslate nohighlight">\(\nu\left(\Omega^{(0)}\right)\)</span> resp. <span class="math notranslate nohighlight">\(\nu\left(\Omega^{(c)}\right)\)</span> can be considered as
the numbers of states in those sets.</p>
</section>
</section>


          </div>
          
        </div>
      </div>  <!-- This includes the original document content -->

    <!-- Back to Top Button -->
    <button onclick="scrollToTop()" id="back-to-top" title="Go to top">↑</button>
  </div>

      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Monalisa</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="1_quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_contents.html">Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_examples.html">Examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="4_api.html">API</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="5_docker.html">Docker for Monalisa</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_ack_contribution.html">Acknowledgment and Authors</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="7_discussions.html">Discussion</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Sketching a Classical Thermodynamic Theory of Information for MRI Reconstructions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#iterative-reconstructions-and-phase-space">Iterative Reconstructions and Phase Space</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-heat-engine-and-the-computer">The Heat Engine and the Computer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#connections-with-the-theory-of-information">Connections with the Theory of Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#discussion">Discussion</a></li>
</ul>
</li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="7_discussions.html">Discussion</a><ul>
      <li>Previous: <a href="7_discussions.html" title="previous chapter">Discussion</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Bastien Milani.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/7-1_thermodyn_info.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>